{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5de378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc master running locally\n",
    "sc.master\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f465503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"/user1/Suicide_Detection.csv\"\n",
    "df = spark.read.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f724f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a62d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+\n",
      "|  ID|                Text|      Label|\n",
      "+----+--------------------+-----------+\n",
      "|null|                text|      class|\n",
      "|   2|Ex Wife Threateni...|    suicide|\n",
      "|   3|Am I weird I don'...|non-suicide|\n",
      "|   4|\"Finally 2020 is ...|non-suicide|\n",
      "|   8|i need helpjust h...|    suicide|\n",
      "+----+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns\n",
    "df = df.withColumnRenamed('_c0', 'ID')      # Rename _c0 to ID\n",
    "df = df.withColumnRenamed('_c1', 'Text')    # Rename _c1 to Text\n",
    "df = df.withColumnRenamed('_c2', 'Label')   # Rename _c2 to Label\n",
    "\n",
    "# Show the DataFrame with renamed columns\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8b6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 664905\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count Rows\n",
    "row_count = df.count()\n",
    "print(\"Total Rows:\", row_count)\n",
    "\n",
    "# Get Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# \"ID\" column datatype changing\n",
    "df = df.withColumn(\"ID\", col(\"ID\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb78d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null values in any column\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4748cb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|                Text|      Label|\n",
      "+---+--------------------+-----------+\n",
      "|  2|Ex Wife Threateni...|    suicide|\n",
      "|  3|Am I weird I don'...|non-suicide|\n",
      "|  4|\"Finally 2020 is ...|non-suicide|\n",
      "|  8|i need helpjust h...|    suicide|\n",
      "| 18|My life is over a...|    suicide|\n",
      "| 19|I took the rest o...|    suicide|\n",
      "| 21|Do you think gett...|    suicide|\n",
      "| 23|Been arrested - f...|    suicide|\n",
      "| 24|Fuck the verizon ...|non-suicide|\n",
      "| 31|Me: I know I have...|non-suicide|\n",
      "| 37|Guys I want frien...|non-suicide|\n",
      "| 39|I’m trashLol I no...|    suicide|\n",
      "| 41|What is the best ...|    suicide|\n",
      "| 43|Today's fact is R...|non-suicide|\n",
      "| 44|I feel like I am ...|    suicide|\n",
      "| 45|Is it worth it?Is...|    suicide|\n",
      "| 47|I triple nipple d...|non-suicide|\n",
      "| 54|Hey, im gonna sle...|non-suicide|\n",
      "| 57|I learnt a new sk...|non-suicide|\n",
      "| 62|Why does no one u...|non-suicide|\n",
      "+---+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to include only 'suicide' and 'non-suicide' labels\n",
    "df = df.filter((col('Label') == 'suicide') | (col('Label') == 'non-suicide'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49e8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      Label|count|\n",
      "+-----------+-----+\n",
      "|non-suicide|76506|\n",
      "|    suicide|53612|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df' is your DataFrame and 'Label' is the column of interest\n",
    "label_counts = df.groupBy(\"Label\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "# Show the label counts\n",
    "label_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c750ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|ID |Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |Label      |\n",
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|2  |Ex Wife Threatening SuicideRecently I left my wife for good because she has cheated on me twice and lied to me so much that I have decided to refuse to go back to her. As of a few days ago, she began threatening suicide. I have tirelessly spent these paat few days talking her out of it and she keeps hesitating because she wants to believe I'll come back. I know a lot of people will threaten this in order to get their way, but what happens if she really does? What do I do and how am I supposed to handle her death on my hands? I still love my wife but I cannot deal with getting cheated on again and constantly feeling insecure. I'm worried today may be the day she does it and I hope so much it doesn't happen.|suicide    |\n",
      "|3  |Am I weird I don't get affected by compliments if it's coming from someone I know irl but I feel really good when internet strangers do it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |non-suicide|\n",
      "+---+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cd238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ID |Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |Label      |tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2  |Ex Wife Threatening SuicideRecently I left my wife for good because she has cheated on me twice and lied to me so much that I have decided to refuse to go back to her. As of a few days ago, she began threatening suicide. I have tirelessly spent these paat few days talking her out of it and she keeps hesitating because she wants to believe I'll come back. I know a lot of people will threaten this in order to get their way, but what happens if she really does? What do I do and how am I supposed to handle her death on my hands? I still love my wife but I cannot deal with getting cheated on again and constantly feeling insecure. I'm worried today may be the day she does it and I hope so much it doesn't happen.                                                                                                                                                                                                                                                                                                                                                                                                                 |suicide    |[ex, wife, threatening, suiciderecently, i, left, my, wife, for, good, because, she, has, cheated, on, me, twice, and, lied, to, me, so, much, that, i, have, decided, to, refuse, to, go, back, to, her., as, of, a, few, days, ago,, she, began, threatening, suicide., i, have, tirelessly, spent, these, paat, few, days, talking, her, out, of, it, and, she, keeps, hesitating, because, she, wants, to, believe, i'll, come, back., i, know, a, lot, of, people, will, threaten, this, in, order, to, get, their, way,, but, what, happens, if, she, really, does?, what, do, i, do, and, how, am, i, supposed, to, handle, her, death, on, my, hands?, i, still, love, my, wife, but, i, cannot, deal, with, getting, cheated, on, again, and, constantly, feeling, insecure., i'm, worried, today, may, be, the, day, she, does, it, and, i, hope, so, much, it, doesn't, happen.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|3  |Am I weird I don't get affected by compliments if it's coming from someone I know irl but I feel really good when internet strangers do it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |non-suicide|[am, i, weird, i, don't, get, affected, by, compliments, if, it's, coming, from, someone, i, know, irl, but, i, feel, really, good, when, internet, strangers, do, it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|4  |\"Finally 2020 is almost over... So I can never hear \"\"2020 has been a bad year\"\" ever again. I swear to fucking God it's so annoying\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |non-suicide|[\"finally, 2020, is, almost, over..., so, i, can, never, hear, \"\"2020, has, been, a, bad, year\"\", ever, again., i, swear, to, fucking, god, it's, so, annoying\"]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|8  |i need helpjust help me im crying so hard                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |suicide    |[i, need, helpjust, help, me, im, crying, so, hard]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|18 |My life is over at 20 years oldHello all. I am a 20 year old balding male. My hairline is trash and to make matters worse my head is HUGE. I have bipolar, depression and crippling social anxiety. Balding has been the cherry on top. I wear a hat 24/7 even in my room when I’m alone because I can’t stop thinking about it. I pop Xanax all day to try and numb the pain and it works for a little bit but it all comes crashing back twice as hard once I come down. I don’t know how to communicate with people anymore and I don’t know how to keep a relationship. I used to be one of the “popular kids” but after my dad passed away I feel into a deep dark hole. I’ve been arrested numerous times, been in rehab, mental hospitals, you name it. The only reason I haven’t killed myself yet is because of my mom and brothers. If I didn’t have them I’d be dead long ago. But it’s getting to the point where even their love and support isn’t going to be enough to keep me alive anymore. I’m either going to be the guy who killed himself, or the guy who went bald and 20 and looks like a child molestor. Which one would you choose?|suicide    |[my, life, is, over, at, 20, years, oldhello, all., i, am, a, 20, year, old, balding, male., my, hairline, is, trash, and, to, make, matters, worse, my, head, is, huge., i, have, bipolar,, depression, and, crippling, social, anxiety., balding, has, been, the, cherry, on, top., i, wear, a, hat, 24/7, even, in, my, room, when, i’m, alone, because, i, can’t, stop, thinking, about, it., i, pop, xanax, all, day, to, try, and, numb, the, pain, and, it, works, for, a, little, bit, but, it, all, comes, crashing, back, twice, as, hard, once, i, come, down., i, don’t, know, how, to, communicate, with, people, anymore, and, i, don’t, know, how, to, keep, a, relationship., i, used, to, be, one, of, the, “popular, kids”, but, after, my, dad, passed, away, i, feel, into, a, deep, dark, hole., i’ve, been, arrested, numerous, times,, been, in, rehab,, mental, hospitals,, you, name, it., the, only, reason, i, haven’t, killed, myself, yet, is, because, of, my, mom, and, brothers., if, i, didn’t, have, them, i’d, be, dead, long, ago., but, it’s, getting, to, the, point, where, even, their, love, and, support, isn’t, going, to, be, enough, to, keep, me, alive, anymore., i’m, either, going, to, be, the, guy, who, killed, himself,, or, the, guy, who, went, bald, and, 20, and, looks, like, a, child, molestor., which, one, would, you, choose?]|\n",
      "|19 |I took the rest of my sleeping pills and my painkillersI can’t wait for it to end, I’ve struggled for the past 6 years and I’m finally ending it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |suicide    |[i, took, the, rest, of, my, sleeping, pills, and, my, painkillersi, can’t, wait, for, it, to, end,, i’ve, struggled, for, the, past, 6, years, and, i’m, finally, ending, it.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|21 |Do you think getting hit by a train would be painful?Guns are hard to come by in my country but trains are not. I just don't want to suffer though, do you think this would be a painless method of suicide?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |suicide    |[do, you, think, getting, hit, by, a, train, would, be, painful?guns, are, hard, to, come, by, in, my, country, but, trains, are, not., i, just, don't, want, to, suffer, though,, do, you, think, this, would, be, a, painless, method, of, suicide?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|23 |Been arrested - feeling suicidalEdit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |suicide    |[been, arrested, -, feeling, suicidaledit]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|24 |Fuck the verizon smart family app I can’t even watch porn privately anymore wtf why is that a feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |non-suicide|[fuck, the, verizon, smart, family, app, i, can’t, even, watch, porn, privately, anymore, wtf, why, is, that, a, feature]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|31 |Me: I know I have a really toxic house and I do my best to cope with with it by going to school, etc Rona: hahahaha, stay at home forcefully go brrrrrrrrr                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |non-suicide|[me:, i, know, i, have, a, really, toxic, house, and, i, do, my, best, to, cope, with, with, it, by, going, to, school,, etc, rona:, hahahaha,, stay, at, home, forcefully, go, brrrrrrrrr]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|37 |Guys I want friends That’s it , I’m alone and don’t talk to anyone dm me or anything, I’m just tired of only talking to my dad and sister, literally only my dad and sister , I like animated series but I’m flexible to anything the last series I watch was Santa Clarita’s diet and the last animated series I watch was the hollow and shera ( I was watching them at the same time ), and I have a very extended music repertoire I can draw you anything you ask ( or at least I’ll try ) you can text me at any hour of the day , I pretty much only do that , I have weird family anecdotes and stories I can tell you  , but if you are not interested in anything I mentioned it doesn’t matter I’m just here to be a friend                                                                                                                                                                                                                                                                                                                                                                                                                      |non-suicide|[guys, i, want, friends, that’s, it, ,, i’m, alone, and, don’t, talk, to, anyone, dm, me, or, anything,, i’m, just, tired, of, only, talking, to, my, dad, and, sister,, literally, only, my, dad, and, sister, ,, i, like, animated, series, but, i’m, flexible, to, anything, the, last, series, i, watch, was, santa, clarita’s, diet, and, the, last, animated, series, i, watch, was, the, hollow, and, shera, (, i, was, watching, them, at, the, same, time, ),, and, i, have, a, very, extended, music, repertoire, i, can, draw, you, anything, you, ask, (, or, at, least, i’ll, try, ), you, can, text, me, at, any, hour, of, the, day, ,, i, pretty, much, only, do, that, ,, i, have, weird, family, anecdotes, and, stories, i, can, tell, you, , ,, but, if, you, are, not, interested, in, anything, i, mentioned, it, doesn’t, matter, i’m, just, here, to, be, a, friend]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|39 |I’m trashLol I normally cringe at the self loathing posts here but honestly I’m such trash. Like literally everything about me.  I just wish I could muster up the courage to just follow through. This is it and I’m okay with that that:  everyday here is worst than the last.  I appreciate this community for letting me know I’m not alone.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |suicide    |[i’m, trashlol, i, normally, cringe, at, the, self, loathing, posts, here, but, honestly, i’m, such, trash., like, literally, everything, about, me., , i, just, wish, i, could, muster, up, the, courage, to, just, follow, through., this, is, it, and, i’m, okay, with, that, that:, , everyday, here, is, worst, than, the, last., , i, appreciate, this, community, for, letting, me, know, i’m, not, alone.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|41 |What is the best way to do it?I’m not looking to be talked out of it. What would be the most effective, easiest way to go?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |suicide    |[what, is, the, best, way, to, do, it?i’m, not, looking, to, be, talked, out, of, it., what, would, be, the, most, effective,, easiest, way, to, go?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|43 |Today's fact is Reddit awards are expensive emojis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |non-suicide|[today's, fact, is, reddit, awards, are, expensive, emojis]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|44 |I feel like I am drowningI used to go to school at a state university, then I had to drop out for financial issues. It's been a year since then and I haven't moved anywhere in my life. I have one job (15 hours a week, so I don't make enough money for essentials like food, soap, toothpaste, etc..) and I am trying to find another job but I can't find one. I feel like I'm going to be stuck in this hole forever and  part of me just wants everything to be over. I apply to 5-6 jobs a day and make sure to call back but all I get is an explanation as to why they don't need me. I also haven't been able to afford a haircut for 3 months and I'm self-conscious about it because my hair never gets long and it feels gross. I spent all of this month with my bank account overdrafted and I have no clue how I will pay rent. Sorry for rambling, but I can't stop thinking of killing myself and I really want help.                                                                                                                                                                                                                    |suicide    |[i, feel, like, i, am, drowningi, used, to, go, to, school, at, a, state, university,, then, i, had, to, drop, out, for, financial, issues., it's, been, a, year, since, then, and, i, haven't, moved, anywhere, in, my, life., i, have, one, job, (15, hours, a, week,, so, i, don't, make, enough, money, for, essentials, like, food,, soap,, toothpaste,, etc..), and, i, am, trying, to, find, another, job, but, i, can't, find, one., i, feel, like, i'm, going, to, be, stuck, in, this, hole, forever, and, , part, of, me, just, wants, everything, to, be, over., i, apply, to, 5-6, jobs, a, day, and, make, sure, to, call, back, but, all, i, get, is, an, explanation, as, to, why, they, don't, need, me., i, also, haven't, been, able, to, afford, a, haircut, for, 3, months, and, i'm, self-conscious, about, it, because, my, hair, never, gets, long, and, it, feels, gross., i, spent, all, of, this, month, with, my, bank, account, overdrafted, and, i, have, no, clue, how, i, will, pay, rent., sorry, for, rambling,, but, i, can't, stop, thinking, of, killing, myself, and, i, really, want, help.]                                                                                                                                                                                                                                                          |\n",
      "|45 |Is it worth it?Is all the trouble, work and anxiety really worth living for.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |suicide    |[is, it, worth, it?is, all, the, trouble,, work, and, anxiety, really, worth, living, for.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|47 |I triple nipple dock dare you To ask out your crush, or if you're taken tell them that you love them                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |non-suicide|[i, triple, nipple, dock, dare, you, to, ask, out, your, crush,, or, if, you're, taken, tell, them, that, you, love, them]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|54 |Hey, im gonna sleep with socks Whatcha gonna do? Put them off?! Good luck ima gonna sleep with warm feet                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |non-suicide|[hey,, im, gonna, sleep, with, socks, whatcha, gonna, do?, put, them, off?!, good, luck, ima, gonna, sleep, with, warm, feet]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|57 |I learnt a new skill today! I learnt how to change a light bulb, only thing that happened was I cocked up the first time and it went out after 5 minutes, so I tightened it a bit and it now works. My room had been lightless for quite a long time and I finally have a bulb in the light.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |non-suicide|[i, learnt, a, new, skill, today!, i, learnt, how, to, change, a, light, bulb,, only, thing, that, happened, was, i, cocked, up, the, first, time, and, it, went, out, after, 5, minutes,, so, i, tightened, it, a, bit, and, it, now, works., my, room, had, been, lightless, for, quite, a, long, time, and, i, finally, have, a, bulb, in, the, light.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|62 |Why does no one use the email function of reddit It's kinda sad, so underappreciated, edit- have y'all never used the email function, how uncultured                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |non-suicide|[why, does, no, one, use, the, email, function, of, reddit, it's, kinda, sad,, so, underappreciated,, edit-, have, y'all, never, used, the, email, function,, how, uncultured]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Create a Tokenizer object\n",
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"tokens\")\n",
    "\n",
    "# Tokenize the \"processed_text\" column\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8c1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[ex, wife, threatening, suiciderecently, i, left, my, wife, for, good, because, she, has, cheated, on, me, twice, and, lied, to, me, so, much, that, i, have, decided, to, refuse, to, go, back, to, her., as, of, a, few, days, ago,, she, began, threatening, suicide., i, have, tirelessly, spent, these, paat, few, days, talking, her, out, of, it, and, she, keeps, hesitating, because, she, wants, to, believe, i'll, come, back., i, know, a, lot, of, people, will, threaten, this, in, order, to, get, their, way,, but, what, happens, if, she, really, does?, what, do, i, do, and, how, am, i, supposed, to, handle, her, death, on, my, hands?, i, still, love, my, wife, but, i, cannot, deal, with, getting, cheated, on, again, and, constantly, feeling, insecure., i'm, worried, today, may, be, the, day, she, does, it, and, i, hope, so, much, it, doesn't, happen.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[am, i, weird, i, don't, get, affected, by, compliments, if, it's, coming, from, someone, i, know, irl, but, i, feel, really, good, when, internet, strangers, do, it]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|[\"finally, 2020, is, almost, over..., so, i, can, never, hear, \"\"2020, has, been, a, bad, year\"\", ever, again., i, swear, to, fucking, god, it's, so, annoying\"]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|[i, need, helpjust, help, me, im, crying, so, hard]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[my, life, is, over, at, 20, years, oldhello, all., i, am, a, 20, year, old, balding, male., my, hairline, is, trash, and, to, make, matters, worse, my, head, is, huge., i, have, bipolar,, depression, and, crippling, social, anxiety., balding, has, been, the, cherry, on, top., i, wear, a, hat, 24/7, even, in, my, room, when, i’m, alone, because, i, can’t, stop, thinking, about, it., i, pop, xanax, all, day, to, try, and, numb, the, pain, and, it, works, for, a, little, bit, but, it, all, comes, crashing, back, twice, as, hard, once, i, come, down., i, don’t, know, how, to, communicate, with, people, anymore, and, i, don’t, know, how, to, keep, a, relationship., i, used, to, be, one, of, the, “popular, kids”, but, after, my, dad, passed, away, i, feel, into, a, deep, dark, hole., i’ve, been, arrested, numerous, times,, been, in, rehab,, mental, hospitals,, you, name, it., the, only, reason, i, haven’t, killed, myself, yet, is, because, of, my, mom, and, brothers., if, i, didn’t, have, them, i’d, be, dead, long, ago., but, it’s, getting, to, the, point, where, even, their, love, and, support, isn’t, going, to, be, enough, to, keep, me, alive, anymore., i’m, either, going, to, be, the, guy, who, killed, himself,, or, the, guy, who, went, bald, and, 20, and, looks, like, a, child, molestor., which, one, would, you, choose?]|\n",
      "|[i, took, the, rest, of, my, sleeping, pills, and, my, painkillersi, can’t, wait, for, it, to, end,, i’ve, struggled, for, the, past, 6, years, and, i’m, finally, ending, it.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|[do, you, think, getting, hit, by, a, train, would, be, painful?guns, are, hard, to, come, by, in, my, country, but, trains, are, not., i, just, don't, want, to, suffer, though,, do, you, think, this, would, be, a, painless, method, of, suicide?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|[been, arrested, -, feeling, suicidaledit]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[fuck, the, verizon, smart, family, app, i, can’t, even, watch, porn, privately, anymore, wtf, why, is, that, a, feature]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[me:, i, know, i, have, a, really, toxic, house, and, i, do, my, best, to, cope, with, with, it, by, going, to, school,, etc, rona:, hahahaha,, stay, at, home, forcefully, go, brrrrrrrrr]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[guys, i, want, friends, that’s, it, ,, i’m, alone, and, don’t, talk, to, anyone, dm, me, or, anything,, i’m, just, tired, of, only, talking, to, my, dad, and, sister,, literally, only, my, dad, and, sister, ,, i, like, animated, series, but, i’m, flexible, to, anything, the, last, series, i, watch, was, santa, clarita’s, diet, and, the, last, animated, series, i, watch, was, the, hollow, and, shera, (, i, was, watching, them, at, the, same, time, ),, and, i, have, a, very, extended, music, repertoire, i, can, draw, you, anything, you, ask, (, or, at, least, i’ll, try, ), you, can, text, me, at, any, hour, of, the, day, ,, i, pretty, much, only, do, that, ,, i, have, weird, family, anecdotes, and, stories, i, can, tell, you, , ,, but, if, you, are, not, interested, in, anything, i, mentioned, it, doesn’t, matter, i’m, just, here, to, be, a, friend]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[i’m, trashlol, i, normally, cringe, at, the, self, loathing, posts, here, but, honestly, i’m, such, trash., like, literally, everything, about, me., , i, just, wish, i, could, muster, up, the, courage, to, just, follow, through., this, is, it, and, i’m, okay, with, that, that:, , everyday, here, is, worst, than, the, last., , i, appreciate, this, community, for, letting, me, know, i’m, not, alone.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[what, is, the, best, way, to, do, it?i’m, not, looking, to, be, talked, out, of, it., what, would, be, the, most, effective,, easiest, way, to, go?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|[today's, fact, is, reddit, awards, are, expensive, emojis]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[i, feel, like, i, am, drowningi, used, to, go, to, school, at, a, state, university,, then, i, had, to, drop, out, for, financial, issues., it's, been, a, year, since, then, and, i, haven't, moved, anywhere, in, my, life., i, have, one, job, (15, hours, a, week,, so, i, don't, make, enough, money, for, essentials, like, food,, soap,, toothpaste,, etc..), and, i, am, trying, to, find, another, job, but, i, can't, find, one., i, feel, like, i'm, going, to, be, stuck, in, this, hole, forever, and, , part, of, me, just, wants, everything, to, be, over., i, apply, to, 5-6, jobs, a, day, and, make, sure, to, call, back, but, all, i, get, is, an, explanation, as, to, why, they, don't, need, me., i, also, haven't, been, able, to, afford, a, haircut, for, 3, months, and, i'm, self-conscious, about, it, because, my, hair, never, gets, long, and, it, feels, gross., i, spent, all, of, this, month, with, my, bank, account, overdrafted, and, i, have, no, clue, how, i, will, pay, rent., sorry, for, rambling,, but, i, can't, stop, thinking, of, killing, myself, and, i, really, want, help.]                                                                                                                                                                                                                                                          |\n",
      "|[is, it, worth, it?is, all, the, trouble,, work, and, anxiety, really, worth, living, for.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|[i, triple, nipple, dock, dare, you, to, ask, out, your, crush,, or, if, you're, taken, tell, them, that, you, love, them]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[hey,, im, gonna, sleep, with, socks, whatcha, gonna, do?, put, them, off?!, good, luck, ima, gonna, sleep, with, warm, feet]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|[i, learnt, a, new, skill, today!, i, learnt, how, to, change, a, light, bulb,, only, thing, that, happened, was, i, cocked, up, the, first, time, and, it, went, out, after, 5, minutes,, so, i, tightened, it, a, bit, and, it, now, works., my, room, had, been, lightless, for, quite, a, long, time, and, i, finally, have, a, bulb, in, the, light.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|[why, does, no, one, use, the, email, function, of, reddit, it's, kinda, sad,, so, underappreciated,, edit-, have, y'all, never, used, the, email, function,, how, uncultured]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the \"tokens\" column\n",
    "token_df = df.select(\"tokens\")\n",
    "\n",
    "token_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35bdf3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+\n",
      "| ID|                Text|      Label|              tokens|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "|  2|Ex Wife Threateni...|    suicide|[ex, wife, threat...|\n",
      "|  3|Am I weird I don'...|non-suicide|[am, i, weird, i,...|\n",
      "|  4|\"Finally 2020 is ...|non-suicide|[\"finally, 2020, ...|\n",
      "|  8|i need helpjust h...|    suicide|[i, need, helpjus...|\n",
      "| 18|My life is over a...|    suicide|[my, life, is, ov...|\n",
      "+---+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d7b1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, desc\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Explode the tokens into separate rows\n",
    "counting_df = df.withColumn(\"word\", explode(df[\"tokens\"]))\n",
    "\n",
    "# Group by words and count their occurrences\n",
    "word_counts = counting_df.groupBy(\"word\").count()\n",
    "\n",
    "# Order the word counts in descending order\n",
    "word_counts = word_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "# Select the top 50 words\n",
    "top_50_words = word_counts.limit(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d17f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   word| count|\n",
      "+-------+------+\n",
      "|      i|476118|\n",
      "|     to|277623|\n",
      "|    and|236422|\n",
      "|    the|167139|\n",
      "|     my|165837|\n",
      "|      a|164026|\n",
      "|     of|106558|\n",
      "|     it| 87695|\n",
      "|     me| 85530|\n",
      "|   just| 83531|\n",
      "|     is| 79251|\n",
      "|     in| 78412|\n",
      "|    but| 77786|\n",
      "|   that| 72752|\n",
      "|   have| 72492|\n",
      "|    for| 70323|\n",
      "|     so| 66496|\n",
      "|   this| 57312|\n",
      "|   like| 54352|\n",
      "|    i'm| 53014|\n",
      "|     be| 51307|\n",
      "|   want| 48788|\n",
      "|   with| 48505|\n",
      "|    you| 47600|\n",
      "|     do| 45516|\n",
      "|    was| 44319|\n",
      "|     on| 43693|\n",
      "|    not| 40132|\n",
      "|    i’m| 39662|\n",
      "|     if| 37987|\n",
      "|   feel| 36059|\n",
      "|   know| 35882|\n",
      "|  about| 34641|\n",
      "|     or| 34344|\n",
      "|    all| 33957|\n",
      "|   what| 33199|\n",
      "|    get| 33182|\n",
      "|  don't| 32684|\n",
      "| filler| 31819|\n",
      "|     no| 30422|\n",
      "|     at| 29498|\n",
      "|because| 27107|\n",
      "|    out| 26843|\n",
      "|     am| 26665|\n",
      "|    how| 26211|\n",
      "|   been| 25391|\n",
      "|    can| 25217|\n",
      "|   they| 25155|\n",
      "|    are| 24378|\n",
      "|       | 24034|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the top 50 words and their counts\n",
    "top_50_words.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e678bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "856c750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|ID |padded_tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |Label      |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|2  |[ex, wife, threatening, suiciderecently, i, left, my, wife, for, good, because, she, has, cheated, on, me, twice, and, lied, to, me, so, much, that, i, have, decided, to, refuse, to, go, back, to, her., as, of, a, few, days, ago,, she, began, threatening, suicide., i, have, tirelessly, spent, these, paat, few, days, talking, her, out, of, it, and, she, keeps, hesitating, because, she, wants, to, believe, i'll, come, back., i, know, a, lot, of, people, will, threaten, this, in, order, to, get, their, way,, but, what, happens, if, she, really, does?, what, do, i, do, and, how, am, i, supposed, to, handle, her, death, on, my, hands?, i, still, love, my, wife, but, i, cannot, deal, with, getting, cheated, on, again, and, constantly, feeling, insecure., i'm, worried, today, may, be, the, day, she, does, it, and, i, hope, so, much, it, doesn't, happen.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |suicide    |\n",
      "|3  |[am, i, weird, i, don't, get, affected, by, compliments, if, it's, coming, from, someone, i, know, irl, but, i, feel, really, good, when, internet, strangers, do, it, , , , , , , , , , , , , , , , , , , , , , , ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |non-suicide|\n",
      "|4  |[\"finally, 2020, is, almost, over..., so, i, can, never, hear, \"\"2020, has, been, a, bad, year\"\", ever, again., i, swear, to, fucking, god, it's, so, annoying\", , , , , , , , , , , , , , , , , , , , , , , , ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |non-suicide|\n",
      "|8  |[i, need, helpjust, help, me, im, crying, so, hard, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |suicide    |\n",
      "|18 |[my, life, is, over, at, 20, years, oldhello, all., i, am, a, 20, year, old, balding, male., my, hairline, is, trash, and, to, make, matters, worse, my, head, is, huge., i, have, bipolar,, depression, and, crippling, social, anxiety., balding, has, been, the, cherry, on, top., i, wear, a, hat, 24/7, even, in, my, room, when, i’m, alone, because, i, can’t, stop, thinking, about, it., i, pop, xanax, all, day, to, try, and, numb, the, pain, and, it, works, for, a, little, bit, but, it, all, comes, crashing, back, twice, as, hard, once, i, come, down., i, don’t, know, how, to, communicate, with, people, anymore, and, i, don’t, know, how, to, keep, a, relationship., i, used, to, be, one, of, the, “popular, kids”, but, after, my, dad, passed, away, i, feel, into, a, deep, dark, hole., i’ve, been, arrested, numerous, times,, been, in, rehab,, mental, hospitals,, you, name, it., the, only, reason, i, haven’t, killed, myself, yet, is, because, of, my, mom, and, brothers., if, i, didn’t, have, them, i’d, be, dead, long, ago., but, it’s, getting, to, the, point, where, even, their, love, and, support, isn’t, going, to, be, enough, to, keep, me, alive, anymore., i’m, either, going, to, be, the, guy, who, killed, himself,, or, the, guy, who, went, bald, and, 20, and, looks, like, a, child, molestor., which, one, would, you, choose?]|suicide    |\n",
      "+---+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Define the maximum sequence length you want\n",
    "max_sequence_length = 50\n",
    "\n",
    "# Define a function to pad sequences\n",
    "def pad_sequence(tokens, length):\n",
    "    return tokens + [\"\"] * (length - len(tokens))\n",
    "\n",
    "# Create a UDF to apply the padding function\n",
    "pad_udf = F.udf(lambda tokens: pad_sequence(tokens, max_sequence_length), T.ArrayType(T.StringType()))\n",
    "\n",
    "# Apply the UDF to pad the sequences\n",
    "df = df.withColumn(\"padded_tokens\", pad_udf(df[\"tokens\"]))\n",
    "\n",
    "# Select the relevant columns: ID, padded_tokens, and Label\n",
    "final_df = df.select(\"ID\", \"padded_tokens\", \"Label\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39a32115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|       padded_tokens|      Label|\n",
      "+---+--------------------+-----------+\n",
      "|  2|[ex, wife, threat...|    suicide|\n",
      "|  3|[am, i, weird, i,...|non-suicide|\n",
      "|  4|[\"finally, 2020, ...|non-suicide|\n",
      "|  8|[i, need, helpjus...|    suicide|\n",
      "| 18|[my, life, is, ov...|    suicide|\n",
      "+---+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6fbd179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+----------+\n",
      "| ID|                Text|      Label|              tokens|       padded_tokens|LabelIndex|\n",
      "+---+--------------------+-----------+--------------------+--------------------+----------+\n",
      "|  2|Ex Wife Threateni...|    suicide|[ex, wife, threat...|[ex, wife, threat...|       1.0|\n",
      "|  3|Am I weird I don'...|non-suicide|[am, i, weird, i,...|[am, i, weird, i,...|       0.0|\n",
      "+---+--------------------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Initialize the StringIndexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"LabelIndex\")\n",
    "\n",
    "# Fit the StringIndexer on your DataFrame\n",
    "indexed_df = label_indexer.fit(df).transform(df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "indexed_df.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7dfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Define Word2Vec parameters\n",
    "word2vec = Word2Vec(\n",
    "    vectorSize=100,       # Set the size of the word vectors\n",
    "    seed=42,              # Set a seed for reproducibility\n",
    "    inputCol=\"padded_tokens\",  # Specify the input column with your tokenized and padded text\n",
    "    outputCol=\"word_vectors\"   # Specify the output column for word vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5e6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the Word2Vec model\n",
    "word2vec_model = word2vec.fit(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd57cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform your DataFrame to include word vectors\n",
    "df = word2vec_model.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa25af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+--------------------+--------------------+----------+--------------------+\n",
      "| ID|                Text|  Label|              tokens|       padded_tokens|LabelIndex|        word_vectors|\n",
      "+---+--------------------+-------+--------------------+--------------------+----------+--------------------+\n",
      "|  2|Ex Wife Threateni...|suicide|[ex, wife, threat...|[ex, wife, threat...|       1.0|[0.10913591500423...|\n",
      "+---+--------------------+-------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f84ff2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"word_vectors\", outputCol=\"scaled_word_vectors\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "105fd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline to apply the scaler\n",
    "pipeline = Pipeline(stages=[scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c72ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit and transform your DataFrame using the pipeline\n",
    "scaled_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47240faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "| ID|                Text|      Label|              tokens|       padded_tokens|LabelIndex|        word_vectors| scaled_word_vectors|\n",
      "+---+--------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "|  2|Ex Wife Threateni...|    suicide|[ex, wife, threat...|[ex, wife, threat...|       1.0|[0.10913591500423...|[1.26365459506834...|\n",
      "|  3|Am I weird I don'...|non-suicide|[am, i, weird, i,...|[am, i, weird, i,...|       0.0|[-0.2058760570921...|[-0.3663998223869...|\n",
      "|  4|\"Finally 2020 is ...|non-suicide|[\"finally, 2020, ...|[\"finally, 2020, ...|       0.0|[-0.2428014789707...|[-0.5574733446064...|\n",
      "|  8|i need helpjust h...|    suicide|[i, need, helpjus...|[i, need, helpjus...|       1.0|[-0.4018417432811...|[-1.3804397978084...|\n",
      "| 18|My life is over a...|    suicide|[my, life, is, ov...|[my, life, is, ov...|       1.0|[0.04078174688246...|[0.90995053437218...|\n",
      "+---+--------------------+-----------+--------------------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7224435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- padded_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- LabelIndex: double (nullable = false)\n",
      " |-- word_vectors: vector (nullable = true)\n",
      " |-- scaled_word_vectors: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45d29262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"scaled_word_vectors\"],  # Use the column with word vectors\n",
    "    outputCol=\"features\"  # Create a new column called \"features\"\n",
    ")\n",
    "\n",
    "# Transform the DataFrame to include the \"features\" column\n",
    "df_with_features = assembler.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3044805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_data, validation_data, test_data = df_with_features.randomSplit([train_ratio, validation_ratio, test_ratio], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "438baa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:13:25.364291: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-01 18:13:28.749658: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-01 18:13:28.749732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-01 18:13:28.773038: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-01 18:13:29.475863: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-01 18:13:29.480612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 18:13:39.991055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a32a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have a Spark DataFrame named df_with_features\n",
    "X_train = scaled_df.select(\"scaled_word_vectors\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f6f4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparkflow as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cbc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming 'scaled_word_vectors' is a column of DenseVector type\n",
    "# You can use the `toArray` function to convert it to a NumPy array\n",
    "# Assuming 'scaled_df' is your DataFrame\n",
    "numpy_array = np.array(scaled_df.select('scaled_word_vectors').rdd.map(lambda x: x[0]).collect())\n",
    "\n",
    "# Now, you can get the shape of the NumPy array\n",
    "shape = numpy_array.shape\n",
    "\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80dd9874",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:30:51,283 WARN execution.CacheManager: Asked to cache already cached data.\n",
      "2023-10-01 18:30:51,292 WARN execution.CacheManager: Asked to cache already cached data.\n",
      "2023-10-01 18:31:07,681 WARN memory.MemoryStore: Not enough space to cache rdd_153_1 in memory! (computed 67.9 MiB so far)\n",
      "2023-10-01 18:31:07,696 WARN storage.BlockManager: Persisting block rdd_153_1 to disk instead.\n",
      "2023-10-01 18:31:14,600 WARN memory.MemoryStore: Not enough space to cache rdd_153_0 in memory! (computed 131.8 MiB so far)\n",
      "2023-10-01 18:31:14,601 WARN storage.BlockManager: Persisting block rdd_153_0 to disk instead.\n",
      "2023-10-01 18:31:14,915 WARN storage.BlockManager: Putting block rdd_170_3 failed due to exception java.lang.ArrayIndexOutOfBoundsException.\n",
      "2023-10-01 18:31:14,916 WARN storage.BlockManager: Block rdd_170_3 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:15,311 WARN memory.MemoryStore: Not enough space to cache rdd_153_2 in memory! (computed 131.8 MiB so far)\n",
      "2023-10-01 18:31:15,312 WARN storage.BlockManager: Persisting block rdd_153_2 to disk instead.\n",
      "2023-10-01 18:31:15,327 ERROR executor.Executor: Exception in task 3.0 in stage 34.0 (TID 62)\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-10-01 18:31:15,395 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 34.0 (TID 62) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-01 18:31:15,400 ERROR scheduler.TaskSetManager: Task 3 in stage 34.0 failed 1 times; aborting job\n",
      "2023-10-01 18:31:15,466 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 34.0 failed 1 times, most recent failure: Lost task 3.0 in stage 34.0 (TID 62) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:31:15,731 WARN storage.BlockManager: Putting block rdd_153_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:15,732 WARN storage.BlockManager: Block rdd_153_1 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:15,735 WARN storage.BlockManager: Putting block rdd_170_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:15,735 WARN storage.BlockManager: Block rdd_170_1 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:15,776 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 34.0 (TID 60) (10.0.2.15 executor driver): TaskKilled (Stage cancelled)\n",
      "[Stage 34:>                                                         (0 + 2) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1407.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 34.0 failed 1 times, most recent failure: Lost task 3.0 in stage 34.0 (TID 62) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10005/1183956254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Step 9: Fit the model and perform hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/util.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;31m# Set local properties in child thread.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLocalProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#  Note: Supporting tuning params in evaluator need update method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1407.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 34.0 failed 1 times, most recent failure: Lost task 3.0 in stage 34.0 (TID 62) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:31:16,596 WARN storage.BlockManager: Putting block rdd_153_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:16,607 WARN storage.BlockManager: Block rdd_153_0 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:16,609 WARN storage.BlockManager: Putting block rdd_170_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:16,610 WARN storage.BlockManager: Block rdd_170_0 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:16,634 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 34.0 (TID 59) (10.0.2.15 executor driver): TaskKilled (Stage cancelled)\n",
      "2023-10-01 18:31:16,952 WARN storage.BlockManager: Putting block rdd_187_3 failed due to exception java.lang.ArrayIndexOutOfBoundsException.\n",
      "2023-10-01 18:31:16,954 WARN storage.BlockManager: Block rdd_187_3 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:16,955 ERROR executor.Executor: Exception in task 3.0 in stage 35.0 (TID 63)\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-10-01 18:31:16,962 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 35.0 (TID 63) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-01 18:31:16,962 ERROR scheduler.TaskSetManager: Task 3 in stage 35.0 failed 1 times; aborting job\n",
      "2023-10-01 18:31:16,972 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 35.0 failed 1 times, most recent failure: Lost task 3.0 in stage 35.0 (TID 63) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:31:17,261 WARN storage.BlockManager: Putting block rdd_153_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:17,262 WARN storage.BlockManager: Block rdd_153_2 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:17,263 WARN storage.BlockManager: Putting block rdd_170_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2023-10-01 18:31:17,263 WARN storage.BlockManager: Block rdd_170_2 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:17,290 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 34.0 (TID 61) (10.0.2.15 executor driver): TaskKilled (Stage cancelled)\n",
      "2023-10-01 18:31:17,920 WARN storage.BlockManager: Putting block rdd_204_3 failed due to exception java.lang.ArrayIndexOutOfBoundsException.\n",
      "2023-10-01 18:31:17,923 WARN storage.BlockManager: Block rdd_204_3 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:17,932 ERROR executor.Executor: Exception in task 3.0 in stage 36.0 (TID 64)\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-10-01 18:31:17,946 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 36.0 (TID 64) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-01 18:31:17,948 ERROR scheduler.TaskSetManager: Task 3 in stage 36.0 failed 1 times; aborting job\n",
      "2023-10-01 18:31:17,961 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 36.0 failed 1 times, most recent failure: Lost task 3.0 in stage 36.0 (TID 64) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 18:31:18,365 WARN storage.BlockManager: Putting block rdd_221_3 failed due to exception java.lang.ArrayIndexOutOfBoundsException.\n",
      "2023-10-01 18:31:18,365 WARN storage.BlockManager: Block rdd_221_3 could not be removed as it was not found on disk or in memory\n",
      "2023-10-01 18:31:18,367 ERROR executor.Executor: Exception in task 3.0 in stage 37.0 (TID 65)\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "2023-10-01 18:31:18,370 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 37.0 (TID 65) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2023-10-01 18:31:18,374 ERROR scheduler.TaskSetManager: Task 3 in stage 37.0 failed 1 times; aborting job\n",
      "2023-10-01 18:31:18,380 ERROR util.Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 37.0 failed 1 times, most recent failure: Lost task 3.0 in stage 37.0 (TID 65) (10.0.2.15 executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\n",
      "\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\n",
      "\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n",
      "\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\tat java.lang.System.arraycopy(Native Method)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4(Layer.scala:665)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$4$adapted(Layer.scala:664)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.ml.ann.DataStacker.$anonfun$stack$3(Layer.scala:664)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1496)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1487)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1310)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assuming you have a Spark DataFrame named df_with_features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"scaled_word_vectors\"],\n",
    "    outputCol=\"new_features\"  # Choose a new name\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Define the Keras RNN model\n",
    "def create_rnn_model():\n",
    "    input_layer = tf.keras.layers.Input(shape=(20, 300))  # Adjust the input shape accordingly\n",
    "    rnn_layer = tf.keras.layers.SimpleRNN(units=64, activation='relu')(input_layer)\n",
    "    output_layer = tf.keras.layers.Dense(units=2, activation='softmax')(rnn_layer)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Step 3: Convert Keras model to TensorFlow SavedModel format\n",
    "# keras_model = create_rnn_model()\n",
    "# tf.saved_model.save(keras_model, '/path/to/saved_model')  # Replace with the desired path\n",
    "\n",
    "# Step 4: Load the TensorFlow SavedModel\n",
    "# inference_graph = tf.saved_model.load('/path/to/saved_model')  # Replace with the path you used above\n",
    "\n",
    "\n",
    "# Step 5: Create a MultilayerPerceptronClassifier as your final classification layer\n",
    "mlp_classifier = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"scaled_word_vectors\",\n",
    "    labelCol=\"LabelIndex\",\n",
    "    layers=[200, 128, 64, 2],  # Adjust the layer configuration as needed\n",
    "    blockSize=128,  # Set your block size\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 6: Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, mlp_classifier])\n",
    "\n",
    "# Step 7: Define the parameter grid for hyperparameter tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(mlp_classifier.blockSize, [64, 128])  # Adjust blockSize values for tuning\n",
    "    .addGrid(mlp_classifier.maxIter, [5, 10])  # Adjust the number of iterations for tuning\n",
    "    .build())\n",
    "\n",
    "# Step 8: Create a TrainValidationSplit with hyperparameter tuning\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=MulticlassClassificationEvaluator(\n",
    "        labelCol=\"LabelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    ),\n",
    "    trainRatio=0.8\n",
    ")\n",
    "\n",
    "# Step 9: Fit the model and perform hyperparameter tuning\n",
    "model = tvs.fit(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Tune the number of units in the first RNN layer\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    \n",
    "    # Input shape for text classification (input_dim is the length of word vectors)\n",
    "    input_shape = (200, 1)  # Replace input_dim with the actual length of your word vectors\n",
    "    \n",
    "    model.add(layers.SimpleRNN(units=hp_units, activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_accuracy',  # You can choose a different metric\n",
    "                     max_epochs=10,\n",
    "                     factor=2,\n",
    "                     directory='keras_tuner_dir',\n",
    "                     project_name='simple_rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks, e.g., early stopping\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=3)]\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(train_data, validation_data=validation_data, epochs=10, callbacks=callbacks)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a34415",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = df_with_features.select(\"scaled_word_vectors\").collect()\n",
    "predictions = []\n",
    "\n",
    "for row in word_vectors:\n",
    "    vector = tf.convert_to_tensor(row.scaled_word_vectors.toArray(), dtype=tf.float32)\n",
    "    vector = tf.reshape(vector, (1, -1, 1))\n",
    "    prediction = model.predict(vector)\n",
    "    predictions.append(float(tf.argmax(prediction, axis=1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aba61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"scaled_word_vectors\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_with_features = assembler.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model using TensorFlow\n",
    "def create_rnn_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.SimpleRNN(units=best_units, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_udf(rows):\n",
    "    model = create_rnn_model()\n",
    "    predictions = []\n",
    "    for row in rows:\n",
    "        # Assuming you have a \"scaled_word_vectors\" column containing the word vectors\n",
    "        word_vectors = row.scaled_word_vectors\n",
    "        word_vectors = tf.reshape(word_vectors, (1, -1, 1))\n",
    "        prediction = model.predict(word_vectors)\n",
    "        predictions.append(float(tf.argmax(prediction, axis=1)[0]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the UDF\n",
    "predictor_udf = F.udf(predict_udf, FloatType())\n",
    "\n",
    "# Apply the UDF to make predictions\n",
    "predictions_df = df_with_features.withColumn(\"predictions\", predictor_udf(F.struct([df_with_features[x] for x in df_with_features.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad5fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7d5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d8fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da5a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147190c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1844a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b48c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowonspark as tfos\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"YourAppName\").getOrCreate()\n",
    "\n",
    "# Define the best hyperparameters\n",
    "best_units = 64  # Replace with the actual best value found\n",
    "best_learning_rate = 0.0001  # Replace with the actual best value found\n",
    "\n",
    "# Define the RNN model using TensorFlow\n",
    "def create_rnn_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.SimpleRNN(units=best_units, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a UDF to apply the RNN model to Spark DataFrame\n",
    "def predict_udf(rows):\n",
    "    model = create_rnn_model()\n",
    "    predictions = []\n",
    "    for row in rows:\n",
    "        # Assuming you have a \"scaled_word_vectors\" column containing the word vectors\n",
    "        word_vectors = row.scaled_word_vectors\n",
    "        word_vectors = tf.reshape(word_vectors, (1, -1, 1))\n",
    "        prediction = model.predict(word_vectors)\n",
    "        predictions.append(float(tf.argmax(prediction, axis=1)[0]))\n",
    "    return predictions\n",
    "\n",
    "# Register the UDF\n",
    "predictor_udf = F.udf(predict_udf, FloatType())\n",
    "\n",
    "# Apply the UDF to make predictions\n",
    "predictions_df = df_with_features.withColumn(\"predictions\", predictor_udf(F.struct([df_with_features[x] for x in df_with_features.columns])))\n",
    "\n",
    "# Show the predictions\n",
    "predictions_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = final_model.evaluate(test_data)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b71995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b0a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb225a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ef500",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.show(2, truncate=Fals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd3bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac4592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a1e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8772bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b68c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d61512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "lists = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b86b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs with Spark\n",
    "tokenize_udf = udf(tokenize, StringType())\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816688b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, remove stopwords, and lemmatize the \"text\" column\n",
    "df = df.withColumn(\"tokens\", tokenize_udf(df[\"text\"]))\n",
    "df = df.withColumn(\"filtered_tokens\", remove_stopwords_udf(df[\"tokens\"]))\n",
    "df = df.withColumn(\"processed_text\", lemmatize_udf(df[\"filtered_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124c463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"processed_text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e428d8a",
   "metadata": {},
   "source": [
    "First row original Text:\n",
    "\n",
    "Ex Wife Threatening SuicideRecently I left my wife for good because she has cheated on me twice and lied to me so much that I have decided to refuse to go back to her. As of a few days ago, she began threatening suicide. I have tirelessly spent these paat few days talking her out of it and she keeps hesitating because she wants to believe I'll come back. I know a lot of people will threaten this in order to get their way, but what happens if she really does? What do I do and how am I supposed to handle her death on my hands? I still love my wife but I cannot deal with getting cheated on again and constantly feeling insecure. I'm worried today may be the day she does it and I hope so much it doesn't happen.\n",
    "\n",
    "First row after Tokenization, Lemmatization and Removing Stopwords:\n",
    "\n",
    "Ex, Wife, Threatening, SuicideRecently, left, wife, good, cheated, twice, lied, much, decided, refuse, go, back, ., day, ago, ,, began, threatening, suicide, ., tirelessly, spent, paat, day, talking, keep, hesitating, want, believe, 'll, come, back, ., know, lot, people, threaten, order, get, way, ,, happens, really, ?, supposed, handle, death, hand, ?, still, love, wife, deal, getting, cheated, constantly, feeling, insecure, ., 'm, worried, today, may, day, hope, much, n't, happen, ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510edd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\"Text\", \"tokens\", \"filtered_tokens\", \"lemmatized_tokens\"]\n",
    "\n",
    "# Remove the specified columns\n",
    "df = df.drop(*columns_to_remove)\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Reorder the columns\n",
    "df = df.select(\"ID\", \"processed_text\", \"Label\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2ba56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from functools import reduce\n",
    "\n",
    "# List of preprocessing functions\n",
    "preprocessing_functions = [\n",
    "    lambda text: lower(text),                                     # Convert text to lowercase\n",
    "    lambda text: regexp_replace(text, r'\\S+@\\S+', ''),           # Remove email addresses\n",
    "    lambda text: regexp_replace(text, r'<.*?>', ''),             # Remove HTML tags\n",
    "    lambda text: regexp_replace(text, r'[^a-zA-Z0-9\\s,.]', ' '), # Remove special characters except , and .\n",
    "    lambda text: regexp_replace(text, r'[^\\x00-\\x7F]+', ''),     # Remove accented characters\n",
    "    lambda text: regexp_replace(text, r'\\s+', ' ')              # Remove extra spaces\n",
    "]\n",
    "\n",
    "# Apply preprocessing functions to the \"processed_text\" column\n",
    "for func in preprocessing_functions:\n",
    "    df = df.withColumn(\"processed_text\", func(df[\"processed_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8aca2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the updated DataFrame\n",
    "df.show(20, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the labels\n",
    "df = df.filter(col('Label') == 'suicide')\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Create a Tokenizer instance\n",
    "tokenizer = Tokenizer(inputCol=\"processed_text\", outputCol=\"tokens\")\n",
    "\n",
    "# Tokenize the \"processed_text\" column\n",
    "tokenized_df = tokenizer.transform(df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "tokenized_df.select(\"Label\", \"tokens\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c493f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Split the processed_text column into individual words\n",
    "df = df.withColumn(\"words\", split(col(\"processed_text\"), \" \"))\n",
    "\n",
    "# Explode the words column to create a row for each word\n",
    "df = df.withColumn(\"word\", explode(col(\"words\")))\n",
    "\n",
    "# Group by words and count their occurrences\n",
    "word_counts = df.groupBy(\"word\").count()\n",
    "\n",
    "# Order the word counts in descending order\n",
    "word_counts = word_counts.orderBy(col(\"count\").desc())\n",
    "\n",
    "# Show the top 20 words and their counts\n",
    "word_counts.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195f4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['processed_text']\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec, CountVectorizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize the text data\n",
    "tokenizer = Tokenizer(inputCol=\"processed_text\", outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ccebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a vocabulary from the tokens\n",
    "count_vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline to execute these steps\n",
    "pipeline = Pipeline(stages=[tokenizer, count_vectorizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0dfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data using the pipeline\n",
    "model = pipeline.fit(df)\n",
    "transformed_df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1230f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"words\", outputCol=\"word_vectors\")\n",
    "model = word2Vec.fit(wordsData)\n",
    "result = model.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a label column named \"label\" and features column named \"word_vectors\"\n",
    "labeledData = result.select(\"ID\", \"word_vectors\",\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d69b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labeledData.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef52833",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = labeledData.select(\"word_vectors\").rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b982898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86474992",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledData.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = labeledData.select(\"word_vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratios (80% for training, 10% for testing, 10% for validation)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Split the data into training, testing, and validation\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126c25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  The number of rows in each split\n",
    "print(\"Train Data Count: \", train_data.count())\n",
    "print(\"Test Data Count: \", test_data.count())\n",
    "print(\"Validation Data Count: \", validation_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, SimpleRNN, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming num_words is the number of unique words in your vocabulary\n",
    "num_columns = len(X.columns)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=EMBEDDING_DIM,\n",
    "                    input_length=num_columns,\n",
    "                    weights=[gensim_weight_matrix],\n",
    "                    trainable=False))\n",
    "model.add(SimpleRNN(100, return_sequences=True, input_shape=(None, num_words)))  # Input shape adjusted\n",
    "model.add(Dropout(0.2))\n",
    "model.add(SimpleRNN(200, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(SimpleRNN(100, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))  # Output dimension adjusted for binary classification\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('./model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b84dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_embedding = model.fit(train_data, train_labels, \n",
    "                                epochs = 25, batch_size = 128, \n",
    "                                validation_data=(validation_data, validation_labels),\n",
    "                                verbose = 1, callbacks= [es, mc]  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

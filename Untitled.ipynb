{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5de378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4c49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64c0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f465503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"/user1/Suicide_Detection.csv\"\n",
    "df = spark.read.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f724f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a62d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+\n",
      "|  ID|                Text|      Label|\n",
      "+----+--------------------+-----------+\n",
      "|null|                text|      class|\n",
      "|   2|Ex Wife Threateni...|    suicide|\n",
      "|   3|Am I weird I don'...|non-suicide|\n",
      "|   4|\"Finally 2020 is ...|non-suicide|\n",
      "|   8|i need helpjust h...|    suicide|\n",
      "+----+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns\n",
    "df = df.withColumnRenamed('_c0', 'ID')      # Rename _c0 to ID\n",
    "df = df.withColumnRenamed('_c1', 'Text')    # Rename _c1 to Text\n",
    "df = df.withColumnRenamed('_c2', 'Label')   # Rename _c2 to Label\n",
    "\n",
    "# Show the DataFrame with renamed columns\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8b6e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 664905\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count Rows\n",
    "row_count = df.count()\n",
    "print(\"Total Rows:\", row_count)\n",
    "\n",
    "# Get Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bbdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 65758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df[df['Label'] == 'suicide']\n",
    "\n",
    "row_count = df.count()\n",
    "print(\"Total Rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# \"ID\" column datatype changing\n",
    "df = df.withColumn(\"ID\", col(\"ID\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb78d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with null values in any column\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9382a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 53612\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count Rows\n",
    "row_count = df.count()\n",
    "print(\"Total Rows:\", row_count)\n",
    "\n",
    "# Get Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20d61512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "lists = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ba7635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Register UDFs with Spark\n",
    "tokenize_udf = udf(tokenize, StringType())\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "\n",
    "# Tokenize, remove stopwords, and lemmatize the \"text\" column\n",
    "df = df.withColumn(\"tokens\", tokenize_udf(df[\"text\"]))\n",
    "df = df.withColumn(\"filtered_tokens\", remove_stopwords_udf(df[\"tokens\"]))\n",
    "df = df.withColumn(\"processed_text\", lemmatize_udf(df[\"filtered_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b124c463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|processed_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Ex, Wife, Threatening, SuicideRecently, left, wife, good, cheated, twice, lied, much, decided, refuse, go, back, ., day, ago, ,, began, threatening, suicide, ., tirelessly, spent, paat, day, talking, keep, hesitating, want, believe, 'll, come, back, ., know, lot, people, threaten, order, get, way, ,, happens, really, ?, supposed, handle, death, hand, ?, still, love, wife, deal, getting, cheated, constantly, feeling, insecure, ., 'm, worried, today, may, day, hope, much, n't, happen, .]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"processed_text\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e428d8a",
   "metadata": {},
   "source": [
    "First row original Text:\n",
    "\n",
    "Ex Wife Threatening SuicideRecently I left my wife for good because she has cheated on me twice and lied to me so much that I have decided to refuse to go back to her. As of a few days ago, she began threatening suicide. I have tirelessly spent these paat few days talking her out of it and she keeps hesitating because she wants to believe I'll come back. I know a lot of people will threaten this in order to get their way, but what happens if she really does? What do I do and how am I supposed to handle her death on my hands? I still love my wife but I cannot deal with getting cheated on again and constantly feeling insecure. I'm worried today may be the day she does it and I hope so much it doesn't happen.\n",
    "\n",
    "First row after Tokenization, Lemmatization and Removing Stopwords:\n",
    "\n",
    "Ex, Wife, Threatening, SuicideRecently, left, wife, good, cheated, twice, lied, much, decided, refuse, go, back, ., day, ago, ,, began, threatening, suicide, ., tirelessly, spent, paat, day, talking, keep, hesitating, want, believe, 'll, come, back, ., know, lot, people, threaten, order, get, way, ,, happens, really, ?, supposed, handle, death, hand, ?, still, love, wife, deal, getting, cheated, constantly, feeling, insecure, ., 'm, worried, today, may, day, hope, much, n't, happen, ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510edd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "| ID|                Text|  Label|              tokens|     filtered_tokens|      processed_text|\n",
      "+---+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "|  2|Ex Wife Threateni...|suicide|[Ex, Wife, Threat...|[Ex, Wife, Threat...|[Ex, Wife, Threat...|\n",
      "|  8|i need helpjust h...|suicide|[i, need, helpjus...|[need, helpjust, ...|[need, helpjust, ...|\n",
      "| 18|My life is over a...|suicide|[My, life, is, ov...|[life, 20, years,...|[life, 20, year, ...|\n",
      "| 19|I took the rest o...|suicide|[I, took, the, re...|[took, rest, slee...|[took, rest, slee...|\n",
      "| 21|Do you think gett...|suicide|[Do, you, think, ...|[think, getting, ...|[think, getting, ...|\n",
      "+---+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f27cb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| ID|  Label|      processed_text|\n",
      "+---+-------+--------------------+\n",
      "|  2|suicide|[Ex, Wife, Threat...|\n",
      "|  8|suicide|[need, helpjust, ...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\"Text\", \"tokens\", \"filtered_tokens\", \"lemmatized_tokens\"]\n",
    "\n",
    "# Remove the specified columns\n",
    "df = df.drop(*columns_to_remove)\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbd8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| ID|      processed_text|  Label|\n",
      "+---+--------------------+-------+\n",
      "|  2|[Ex, Wife, Threat...|suicide|\n",
      "|  8|[need, helpjust, ...|suicide|\n",
      "+---+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Reorder the columns\n",
    "df = df.select(\"ID\", \"processed_text\", \"Label\")\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dc2ba56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from functools import reduce\n",
    "\n",
    "# List of preprocessing functions\n",
    "preprocessing_functions = [\n",
    "    lambda text: lower(text),                                     # Convert text to lowercase\n",
    "    lambda text: regexp_replace(text, r'\\S+@\\S+', ''),           # Remove email addresses\n",
    "    lambda text: regexp_replace(text, r'<.*?>', ''),             # Remove HTML tags\n",
    "    lambda text: regexp_replace(text, r'[^a-zA-Z0-9\\s]', ' '),   # Remove special characters\n",
    "    lambda text: regexp_replace(text, r'[^\\x00-\\x7F]+', '')     # Remove accented characters\n",
    "]\n",
    "\n",
    "# Apply preprocessing functions to the \"processed_text\" column\n",
    "for func in preprocessing_functions:\n",
    "    df = df.withColumn(\"processed_text\", func(df[\"processed_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da8aca2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| ID|      processed_text|  Label|\n",
      "+---+--------------------+-------+\n",
      "|  2| ex  wife  threat...|suicide|\n",
      "|  8| need  helpjust  ...|suicide|\n",
      "| 18| life  20  year  ...|suicide|\n",
      "| 19| took  rest  slee...|suicide|\n",
      "| 21| think  getting  ...|suicide|\n",
      "| 23| arrested     fee...|suicide|\n",
      "| 39|    trashlol  nor...|suicide|\n",
      "| 41| best  way       ...|suicide|\n",
      "| 44| feel  like  drow...|suicide|\n",
      "| 45| worth     troubl...|suicide|\n",
      "| 67|  ve  become  acc...|suicide|\n",
      "| 69| after   ukif  di...|suicide|\n",
      "| 74| cant  stop  feel...|suicide|\n",
      "| 79| since  aunt   s ...|suicide|\n",
      "| 86| owthe  past  unf...|suicide|\n",
      "| 87| ive  suicidal  l...|suicide|\n",
      "| 90| n t  know  goi  ...|suicide|\n",
      "|113|      s  point  l...|suicide|\n",
      "|120| want  people  kn...|suicide|\n",
      "|127| suicidenote info...|suicide|\n",
      "+---+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the updated DataFrame\n",
    "df.show(20, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3242cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ed78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"processed_text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c696e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+--------------------+\n",
      "| ID|      processed_text|  Label|               words|\n",
      "+---+--------------------+-------+--------------------+\n",
      "|  2| ex  wife  threat...|suicide|[, ex, , wife, , ...|\n",
      "|  8| need  helpjust  ...|suicide|[, need, , helpju...|\n",
      "+---+--------------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "wordsData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1230f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"words\", outputCol=\"word_vectors\")\n",
    "model = word2Vec.fit(wordsData)\n",
    "result = model.transform(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec4b509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a label column named \"label\" and features column named \"word_vectors\"\n",
    "labeledData = result.select(\"ID\", \"word_vectors\",\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55d69b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| ID|        word_vectors|  Label|\n",
      "+---+--------------------+-------+\n",
      "|  2|[0.09567599713777...|suicide|\n",
      "|  8|[0.06323466275352...|suicide|\n",
      "| 18|[0.09200903690944...|suicide|\n",
      "| 19|[0.08549859951936...|suicide|\n",
      "| 21|[0.14425905434414...|suicide|\n",
      "+---+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labeledData.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fef52833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "word_vectors = labeledData.select(\"word_vectors\").rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e02d9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b982898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 18:11:35.149607: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-30 18:11:39.392222: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-30 18:11:39.392375: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-30 18:11:39.413458: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-30 18:11:41.698040: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-30 18:11:41.743709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 18:11:55.661240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455b25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratios (80% for training, 10% for testing, 10% for validation)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Split the data into training, testing, and validation\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The number of rows in each split\n",
    "print(\"Train Data Count: \", train_data.count())\n",
    "print(\"Test Data Count: \", test_data.count())\n",
    "print(\"Validation Data Count: \", validation_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c034dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, SimpleRNN, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c2bf240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, None, 100)         20100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 100)         0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, None, 200)         60200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 200)         0         \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 100)               30100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110602 (432.04 KB)\n",
      "Trainable params: 110602 (432.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Assuming num_words is the number of unique words in your vocabulary\n",
    "num_words = len(word_vectors[0])\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(100, return_sequences=True, input_shape=(None, num_words)))  # Input shape adjusted\n",
    "model.add(Dropout(0.2))\n",
    "model.add(SimpleRNN(200, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(SimpleRNN(100, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax'))  # Output dimension adjusted for binary classification\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0253044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "#EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('./model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b03b0d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4139/1655816950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history_embedding = model.fit(train_data, train_labels, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 verbose = 1, callbacks= [es, mc]  )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "history_embedding = model.fit(train_data, train_labels, \n",
    "                                epochs = 25, batch_size = 128, \n",
    "                                validation_data=(validation_data, validation_labels),\n",
    "                                verbose = 1, callbacks= [es, mc]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b84dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
